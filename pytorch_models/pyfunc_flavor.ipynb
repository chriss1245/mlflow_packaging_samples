{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Packaging a pytorch model with the pyfunc flavor\n",
    "\n",
    "The pyfunc flavor is the most generic flavor in MLflow. You can package whatever you want in it. In this notebook, we will package a pytorch model.\n",
    "\n",
    "* If you need to run several models in one server, you can use the pyfunc flavor to package them all.\n",
    "* If you need to preprocess the data with a custom function, you can use the pyfunc flavor to package the preprocessing function.\n",
    "* If you need to handle complex inputs, outputs  or dependencies, you can use the pyfunc flavor to package the logic.\n",
    "* If you need more flexibility, you can use the pyfunc flavor to package the model and the logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model import SimpleNN, RandomDataset, Trainer\n",
    "from mlflow.pyfunc import save_model, PythonModel, PythonModelContext\n",
    "from mlflow.models.signature import infer_signature\n",
    "import requests\n",
    "from typing import Any\n",
    "import subprocess\n",
    "import torch\n",
    "import shutil\n",
    "import os\n",
    "import signal\n",
    "import time\n",
    "import cloudpickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 10\n",
    "TARGET_SIZE = 2\n",
    "SERVE_PORT = 10001\n",
    "MODEL_PATH = \"model\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "In this example we will train 2 different models and save them in the same pyfunc flavor. The endpoint will use one or the other\n",
    "based on the parameter model: model1 or model2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]/home/ubuntu/miniconda3/envs/mlflow_pytorch/lib/python3.11/site-packages/torch/autograd/graph.py:825: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "loss: 1.02: 100%|██████████| 13/13 [00:00<00:00, 729.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.00: 100%|██████████| 13/13 [00:00<00:00, 963.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.98: 100%|██████████| 13/13 [00:00<00:00, 946.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.95: 100%|██████████| 13/13 [00:00<00:00, 965.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.96: 100%|██████████| 13/13 [00:00<00:00, 931.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.00: 100%|██████████| 13/13 [00:00<00:00, 1043.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.95: 100%|██████████| 13/13 [00:00<00:00, 972.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.96: 100%|██████████| 13/13 [00:00<00:00, 982.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.90: 100%|██████████| 13/13 [00:00<00:00, 996.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.91: 100%|██████████| 13/13 [00:00<00:00, 966.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model1 = SimpleNN(input_size=INPUT_SIZE, output_size=TARGET_SIZE, hidden_size=10)\n",
    "\n",
    "train_dataset = RandomDataset(feat_size=INPUT_SIZE, target_size=TARGET_SIZE, num_samples=100)\n",
    "\n",
    "trainer = Trainer(model1, optimizer=torch.optim.Adam(model1.parameters()), loss_fn=torch.nn.MSELoss())\n",
    "\n",
    "trained_model1 = trainer.train(train_dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.18: 100%|██████████| 13/13 [00:00<00:00, 681.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.11: 100%|██████████| 13/13 [00:00<00:00, 793.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.09: 100%|██████████| 13/13 [00:00<00:00, 374.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 1.09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.06: 100%|██████████| 13/13 [00:00<00:00, 280.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 1.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.05: 100%|██████████| 13/13 [00:00<00:00, 194.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 1.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.02: 100%|██████████| 13/13 [00:00<00:00, 966.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 1.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.05: 100%|██████████| 13/13 [00:00<00:00, 995.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 1.05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.00: 100%|██████████| 13/13 [00:00<00:00, 977.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 1.03: 100%|██████████| 13/13 [00:00<00:00, 1078.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: 0.98: 100%|██████████| 13/13 [00:00<00:00, 1073.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model2 = SimpleNN(input_size=INPUT_SIZE, output_size=TARGET_SIZE, hidden_size=10)\n",
    "\n",
    "train_dataset = RandomDataset(feat_size=INPUT_SIZE, target_size=TARGET_SIZE, num_samples=100)\n",
    "\n",
    "trainer = Trainer(model2, optimizer=torch.optim.Adam(model2.parameters()), loss_fn=torch.nn.MSELoss())\n",
    "\n",
    "trained_model2 = trainer.train(train_dataset, epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Packaging the models\n",
    "In order to package it you can use `save_model` passing a custom class.  This class should implement the methods predict and load_context. We can use the method load context to load the models. The models are saved as artifacts.\n",
    "\n",
    "The code path and the environment it needs. Also... it is advisable to pass the model signature.\n",
    "\n",
    "For the environment I prefer to use conda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchModel(PythonModel):\n",
    "    def __init__(self):\n",
    "        self.model1 = None\n",
    "        self.model2 = None\n",
    "        self.device = None\n",
    "        self.cpu_device = None\n",
    "        self.loaded = False\n",
    "    \n",
    "    def load_context(self, context: PythonModelContext):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.cpu_device = torch.device(\"cpu\")\n",
    "\n",
    "        \n",
    "        self.model1 = torch.load(context.artifacts[\"model1\"], map_location=self.device)\n",
    "        self.model1.eval()\n",
    "        self.model2 = torch.load(context.artifacts[\"model2\"], map_location=self.device)\n",
    "        self.model2.eval()\n",
    "\n",
    "        self.loaded = True\n",
    "    \n",
    "    def predict(self, context: PythonModelContext, model_input: list, params: dict[str, Any]) -> Any:\n",
    "        \n",
    "        if params.get(\"model\") is None:\n",
    "            raise TypeError(\"Model parameter not found\")\n",
    "        \n",
    "        if not self.loaded:\n",
    "            self.load_context(context)\n",
    "\n",
    "        model_input = torch.tensor(model_input, dtype=torch.float32).to(self.device)\n",
    "        \n",
    "        # NOTE: depending in the model parameter, we will use model1 or model2\n",
    "        if params[\"model\"] == \"model1\":\n",
    "            model = self.model1\n",
    "        elif params[\"model\"] == \"model2\":\n",
    "            model = self.model2\n",
    "        else:\n",
    "            raise ValueError(\"Model parameter should be either model1 or model2\")\n",
    "        \n",
    "        # NOTE: you could even run both and take the average, and use a model to predict something\n",
    "        # the next model needs.\n",
    "        \n",
    "        print(self.random_function_made_for_demo())\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(model_input)\n",
    "        return output.cpu().numpy().tolist()\n",
    "        \n",
    "    def random_function_made_for_demo(self):\n",
    "        return \"This is a random function made for demo purposes\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inputs: \n",
       "  [Tensor('float32', (-1, 10))]\n",
       "outputs: \n",
       "  [Tensor('float32', (-1, 2))]\n",
       "params: \n",
       "  ['model': string (default: model1)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# signature\n",
    "input_example = torch.rand(1, INPUT_SIZE)\n",
    "output_example = trained_model1(input_example) # same as trained_model2 since they have the same architecture\n",
    "\n",
    "# NOTE: we are using the params argument which is a dictionary that will be passed to the predict method\n",
    "# this way we can select which model to use in the predict method\n",
    "signature = infer_signature(input_example.numpy(), output_example.detach().numpy(), params={\"model\": \"model1\"})\n",
    "signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda env\n",
    "# you usually want to save this to a file and then load it with mlflow\n",
    "conda_env = {\n",
    "    \"channels\": [\"defaults\"],\n",
    "    \"dependencies\": [\n",
    "        \"python=3.11\",\n",
    "        {\"pip\": [\"mlflow\", \"torch\", \"tqdm\"]}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the models\n",
    "torch.save(trained_model1, \"model1.pth\")\n",
    "torch.save(trained_model2, \"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mlflow_pytorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 2673.23it/s] \n",
      "Downloading artifacts: 100%|██████████| 1/1 [00:00<00:00, 1340.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# save model\n",
    "shutil.rmtree(MODEL_PATH, ignore_errors=True)\n",
    "save_model(path=MODEL_PATH, python_model=PytorchModel(), conda_env=conda_env, code_paths=[\"src\"], artifacts={\"model1\": \"model1.pth\", \"model2\": \"model2.pth\"}, signature=signature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Local serving\n",
    "\n",
    "You can inmediately serve this model and run inference in local. \n",
    "\n",
    "**Note:** With this you may not need docker. It is enough with having correclty set up the conda env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 9/9 [00:00<00:00, 16278.02it/s]\n",
      "2024/12/02 00:34:29 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "2024/12/02 00:34:29 INFO mlflow.pyfunc.backend: === Running command 'exec gunicorn --timeout=60 -b 127.0.0.1:10001 -w 2 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n",
      "[2024-12-02 00:34:29 +0100] [429396] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-12-02 00:34:29 +0100] [429396] [INFO] Listening at: http://127.0.0.1:10001 (429396)\n",
      "[2024-12-02 00:34:29 +0100] [429396] [INFO] Using worker: sync\n",
      "[2024-12-02 00:34:29 +0100] [429397] [INFO] Booting worker with pid: 429397\n",
      "[2024-12-02 00:34:29 +0100] [429398] [INFO] Booting worker with pid: 429398\n"
     ]
    }
   ],
   "source": [
    "# start model server\n",
    "cmd = f\"mlflow models serve -m {MODEL_PATH} -p {SERVE_PORT} --env-manager local --workers 2\" # alternative: --env-manager conda: will create a new conda env\n",
    "process = subprocess.Popen(cmd, shell=True, preexec_fn=os.setsid)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a random function made for demo purposes\n",
      "{'predictions': [[-0.009028077125549316, 0.18052636086940765]]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/miniconda3/envs/mlflow_pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_429266/23546871.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/tmp/ipykernel_429266/23546871.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/home/ubuntu/miniconda3/envs/mlflow_pytorch/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "/tmp/ipykernel_429266/23546871.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/tmp/ipykernel_429266/23546871.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "result = requests.post(f\"http://localhost:{SERVE_PORT}/invocations\", json={\"inputs\": input_example.numpy().tolist(), \"params\": {\"model\": \"model1\"}})\n",
    "print(result.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-02 00:34:32 +0100] [429398] [INFO] Worker exiting (pid: 429398)\n",
      "[2024-12-02 00:34:32 +0100] [429396] [INFO] Handling signal: term\n"
     ]
    }
   ],
   "source": [
    "# stop model server\n",
    "os.killpg(os.getpgid(process.pid), signal.SIGTERM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Docker\n",
    "\n",
    "We also can package the model in docker. It is usually easier this way. It works in your machine and in their machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Packaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-02 00:34:32 +0100] [429397] [INFO] Worker exiting (pid: 429397)\n"
     ]
    }
   ],
   "source": [
    "IMAGE_NAME = \"mlflow-model:pyfunc\" # the name is mlflow-model and the tag is pytorch (you can change it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-02 00:34:32 +0100] [429396] [INFO] Shutting down: Master\n",
      "/home/ubuntu/miniconda3/envs/mlflow_pytorch/lib/python3.11/site-packages/click/core.py:2362: UserWarning: Use of conda is discouraged. If you use it, please ensure that your use of conda complies with Anaconda's terms of service (https://legal.anaconda.com/policies/en/?name=terms-of-service). virtualenv is the recommended tool for environment reproducibility. To suppress this warning, set the MLFLOW_DISABLE_ENV_MANAGER_CONDA_WARNING environment variable to 'TRUE'.\n",
      "  value = self.callback(ctx, self, value)\n",
      "Downloading artifacts: 100%|██████████| 11/11 [00:00<00:00, 21907.57it/s]\n",
      "2024/12/02 00:34:33 INFO mlflow.models.flavor_backend_registry: Selected backend for flavor 'python_function'\n",
      "Downloading artifacts: 100%|██████████| 11/11 [00:00<00:00, 13710.95it/s]\n",
      "2024/12/02 00:34:33 INFO mlflow.pyfunc.backend: Building docker image with name mlflow-model:pyfunc\n",
      "#0 building with \"default\" instance using docker driver\n",
      "\n",
      "#1 [internal] load build definition from Dockerfile\n",
      "#1 transferring dockerfile: 1.95kB done\n",
      "#1 DONE 0.0s\n",
      "\n",
      "#2 [internal] load metadata for docker.io/library/ubuntu:20.04\n",
      "#2 DONE 0.7s\n",
      "\n",
      "#3 [internal] load .dockerignore\n",
      "#3 transferring context: 2B done\n",
      "#3 DONE 0.0s\n",
      "\n",
      "#4 [ 1/15] FROM docker.io/library/ubuntu:20.04@sha256:8e5c4f0285ecbb4ead070431d29b576a530d3166df73ec44affc1cd27555141b\n",
      "#4 DONE 0.0s\n",
      "\n",
      "#5 [internal] load build context\n",
      "#5 transferring context: 20.34kB done\n",
      "#5 DONE 0.0s\n",
      "\n",
      "#6 [ 7/15] RUN pip install mlflow==2.18.0\n",
      "#6 CACHED\n",
      "\n",
      "#7 [ 2/15] RUN apt-get -y update && DEBIAN_FRONTEND=noninteractive TZ=Etc/UTC apt-get install -y --no-install-recommends wget curl nginx ca-certificates bzip2 build-essential cmake git-core\n",
      "#7 CACHED\n",
      "\n",
      "#8 [ 6/15] WORKDIR /opt/mlflow\n",
      "#8 CACHED\n",
      "\n",
      "#9 [ 4/15] RUN bash ./miniconda.sh -b -p /miniconda && rm ./miniconda.sh\n",
      "#9 CACHED\n",
      "\n",
      "#10 [10/15] RUN cp /opt/java/mlflow-scoring-2.18.0.pom /opt/java/pom.xml\n",
      "#10 CACHED\n",
      "\n",
      "#11 [ 3/15] RUN curl --fail -L https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh > miniconda.sh\n",
      "#11 CACHED\n",
      "\n",
      "#12 [ 5/15] RUN apt-get install -y --no-install-recommends openjdk-11-jdk maven\n",
      "#12 CACHED\n",
      "\n",
      "#13 [ 8/15] RUN mvn --batch-mode dependency:copy -Dartifact=org.mlflow:mlflow-scoring:2.18.0:pom -DoutputDirectory=/opt/java\n",
      "#13 CACHED\n",
      "\n",
      "#14 [ 9/15] RUN mvn --batch-mode dependency:copy -Dartifact=org.mlflow:mlflow-scoring:2.18.0:jar -DoutputDirectory=/opt/java/jars\n",
      "#14 CACHED\n",
      "\n",
      "#15 [11/15] RUN cd /opt/java && mvn --batch-mode dependency:copy-dependencies -DoutputDirectory=/opt/java/jars\n",
      "#15 CACHED\n",
      "\n",
      "#16 [12/15] COPY model_dir/model /opt/ml/model\n",
      "#16 DONE 0.0s\n",
      "\n",
      "#17 [13/15] RUN python -c \"from mlflow.models import container as C; C._install_pyfunc_deps('/opt/ml/model', install_mlflow=False, enable_mlserver=False, env_manager='conda');\"\n",
      "#17 1.013 2024/12/01 23:34:35 INFO mlflow.models.container: creating and activating custom environment\n",
      "#17 1.347 /miniconda/lib/python3.12/argparse.py:2006: FutureWarning: `remote_definition` is deprecated and will be removed in 25.9. Use `conda env create --file=URL` instead.\n",
      "#17 1.347   action(self, namespace, argument_values, option_string)\n",
      "#17 1.364 Warning: you have pip-installed dependencies in your environment file, but you do not list pip itself as one of your conda dependencies.  Conda may not use the correct pip to install your packages, and they may end up in the wrong place.  Please add an explicit pip dependency.  I'm adding one for you, but still nagging you.\n",
      "#17 1.579 Channels:\n",
      "#17 1.579  - defaults\n",
      "#17 1.579 Platform: linux-64\n",
      "#17 1.579 Collecting package metadata (repodata.json): ...working... done\n",
      "#17 4.226 Solving environment: ...working... done\n",
      "#17 6.606 \n",
      "#17 6.606 Downloading and Extracting Packages: ...working... done\n",
      "#17 6.606 Preparing transaction: ...working... done\n",
      "#17 6.777 Verifying transaction: ...working... done\n",
      "#17 7.904 Executing transaction: ...working... done\n",
      "#17 11.42 Installing pip dependencies: ...working... Ran pip subprocess with arguments:\n",
      "#17 182.9 ['/miniconda/envs/custom_env/bin/python', '-m', 'pip', 'install', '-U', '-r', '/opt/mlflow/condaenv.h2s_6uwb.requirements.txt', '--exists-action=b']\n",
      "#17 182.9 Pip subprocess output:\n",
      "#17 182.9 Collecting mlflow (from -r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached mlflow-2.18.0-py3-none-any.whl.metadata (29 kB)\n",
      "#17 182.9 Collecting torch (from -r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "#17 182.9 Collecting tqdm (from -r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 3))\n",
      "#17 182.9   Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "#17 182.9 Collecting mlflow-skinny==2.18.0 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached mlflow_skinny-2.18.0-py3-none-any.whl.metadata (30 kB)\n",
      "#17 182.9 Collecting Flask<4 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "#17 182.9 Collecting alembic!=1.10.0,<2 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached alembic-1.14.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "#17 182.9 Collecting docker<8,>=4.0.0 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "#17 182.9 Collecting graphene<4 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "#17 182.9 Collecting markdown<4,>=3.3 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "#17 182.9 Collecting matplotlib<4 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading matplotlib-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "#17 182.9 Collecting numpy<3 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "#17 182.9 Collecting pandas<3 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "#17 182.9 Collecting pyarrow<19,>=4.0.0 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "#17 182.9 Collecting scikit-learn<2 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "#17 182.9 Collecting scipy<2 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "#17 182.9 Collecting sqlalchemy<3,>=1.4.0 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading SQLAlchemy-2.0.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "#17 182.9 Collecting Jinja2<4,>=2.11 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "#17 182.9 Collecting gunicorn<24 (from mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "#17 182.9 Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "#17 182.9 Collecting click<9,>=7.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "#17 182.9 Collecting cloudpickle<4 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "#17 182.9 Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached databricks_sdk-0.38.0-py3-none-any.whl.metadata (38 kB)\n",
      "#17 182.9 Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "#17 182.9 Collecting importlib-metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached importlib_metadata-8.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "#17 182.9 Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached opentelemetry_api-1.28.2-py3-none-any.whl.metadata (1.4 kB)\n",
      "#17 182.9 Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached opentelemetry_sdk-1.28.2-py3-none-any.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting packaging<25 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "#17 182.9 Collecting protobuf<6,>=3.12.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "#17 182.9 Collecting pyyaml<7,>=5.1 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "#17 182.9 Collecting requests<3,>=2.17.3 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "#17 182.9 Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached sqlparse-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "#17 182.9 Collecting filelock (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "#17 182.9 Collecting typing-extensions>=4.8.0 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "#17 182.9 Collecting networkx (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "#17 182.9 Collecting fsspec (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "#17 182.9 Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "#17 182.9 Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "#17 182.9 Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "#17 182.9 Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "#17 182.9 Collecting nvidia-nccl-cu12==2.21.5 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "#17 182.9 Collecting nvidia-nvtx-cu12==12.4.127 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "#17 182.9 Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "#17 182.9 Collecting triton==3.1.0 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "#17 182.9 Collecting sympy==1.13.1 (from torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "#17 182.9 Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 2))\n",
      "#17 182.9   Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "#17 182.9 Collecting Mako (from alembic!=1.10.0,<2->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached Mako-1.3.6-py3-none-any.whl.metadata (2.9 kB)\n",
      "#17 182.9 Collecting urllib3>=1.26.0 (from docker<8,>=4.0.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "#17 182.9 Collecting Werkzeug>=3.1 (from Flask<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "#17 182.9 Collecting itsdangerous>=2.2 (from Flask<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "#17 182.9 Collecting blinker>=1.9 (from Flask<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "#17 182.9 Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached graphql_core-3.2.5-py3-none-any.whl.metadata (10 kB)\n",
      "#17 182.9 Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "#17 182.9 Collecting python-dateutil<3,>=2.7.0 (from graphene<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "#17 182.9 Collecting MarkupSafe>=2.0 (from Jinja2<4,>=2.11->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "#17 182.9 Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "#17 182.9 Collecting cycler>=0.10 (from matplotlib<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "#17 182.9 Collecting fonttools>=4.22.0 (from matplotlib<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading fonttools-4.55.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (164 kB)\n",
      "#17 182.9 Collecting kiwisolver>=1.3.1 (from matplotlib<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.3 kB)\n",
      "#17 182.9 Collecting pillow>=8 (from matplotlib<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "#17 182.9 Collecting pyparsing>=2.3.1 (from matplotlib<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached pyparsing-3.2.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "#17 182.9 Collecting pytz>=2020.1 (from pandas<3->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "#17 182.9 Collecting tzdata>=2022.7 (from pandas<3->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "#17 182.9 Collecting joblib>=1.2.0 (from scikit-learn<2->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "#17 182.9 Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "#17 182.9 Collecting greenlet!=0.4.17 (from sqlalchemy<3,>=1.4.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "#17 182.9 Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached google_auth-2.36.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "#17 182.9 Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "#17 182.9 Collecting zipp>=3.20 (from importlib-metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "#17 182.9 Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "#17 182.9 Collecting opentelemetry-semantic-conventions==0.49b2 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl.metadata (2.3 kB)\n",
      "#17 182.9 Collecting six>=1.5 (from python-dateutil<3,>=2.7.0->graphene<4->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "#17 182.9 Collecting charset-normalizer<4,>=2 (from requests<3,>=2.17.3->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "#17 182.9 Collecting idna<4,>=2.5 (from requests<3,>=2.17.3->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "#17 182.9 Collecting certifi>=2017.4.17 (from requests<3,>=2.17.3->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "#17 182.9 Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "#17 182.9 Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "#17 182.9 Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "#17 182.9 Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "#17 182.9 Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.18.0->mlflow->-r /opt/mlflow/condaenv.h2s_6uwb.requirements.txt (line 1))\n",
      "#17 182.9   Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "#17 182.9 Using cached mlflow-2.18.0-py3-none-any.whl (27.3 MB)\n",
      "#17 182.9 Using cached mlflow_skinny-2.18.0-py3-none-any.whl (5.8 MB)\n",
      "#17 182.9 Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 906.5/906.5 MB 28.7 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 363.4/363.4 MB 32.3 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.8/13.8 MB 34.8 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 24.6/24.6 MB 33.0 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 883.7/883.7 kB 24.4 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 664.8/664.8 MB 31.0 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 211.5/211.5 MB 32.0 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.3/56.3 MB 32.0 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 127.9/127.9 MB 33.6 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.5/207.5 MB 23.9 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 188.7/188.7 MB 32.4 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 30.0 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "#17 182.9 Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.2/6.2 MB 32.2 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 209.5/209.5 MB 34.1 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "#17 182.9 Using cached alembic-1.14.0-py3-none-any.whl (233 kB)\n",
      "#17 182.9 Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "#17 182.9 Using cached flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "#17 182.9 Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "#17 182.9 Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "#17 182.9 Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "#17 182.9 Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "#17 182.9 Downloading matplotlib-3.9.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.3/8.3 MB 31.2 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading numpy-2.1.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 16.3/16.3 MB 32.7 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading pandas-2.2.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.1/13.1 MB 36.3 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading pyarrow-18.1.0-cp311-cp311-manylinux_2_28_x86_64.whl (40.1 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.1/40.1 MB 32.2 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading scikit_learn-1.5.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.3 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.3/13.3 MB 32.6 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading scipy-1.14.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (41.2 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.2/41.2 MB 33.3 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading SQLAlchemy-2.0.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.2/3.2 MB 31.6 MB/s eta 0:00:00\n",
      "#17 182.9 Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "#17 182.9 Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "#17 182.9 Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "#17 182.9 Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.7/1.7 MB 29.7 MB/s eta 0:00:00\n",
      "#17 182.9 Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "#17 182.9 Using cached cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "#17 182.9 Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "#17 182.9 Using cached cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "#17 182.9 Downloading contourpy-1.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (326 kB)\n",
      "#17 182.9 Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "#17 182.9 Using cached databricks_sdk-0.38.0-py3-none-any.whl (575 kB)\n",
      "#17 182.9 Downloading fonttools-4.55.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.9 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.9/4.9 MB 32.1 MB/s eta 0:00:00\n",
      "#17 182.9 Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "#17 182.9 Using cached graphql_core-3.2.5-py3-none-any.whl (203 kB)\n",
      "#17 182.9 Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "#17 182.9 Downloading greenlet-3.1.1-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (602 kB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 602.4/602.4 kB 33.1 MB/s eta 0:00:00\n",
      "#17 182.9 Using cached importlib_metadata-8.5.0-py3-none-any.whl (26 kB)\n",
      "#17 182.9 Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "#17 182.9 Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "#17 182.9 Downloading kiwisolver-1.4.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 35.7 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
      "#17 182.9 Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━���━━━━━━━━━━ 536.2/536.2 kB 31.7 MB/s eta 0:00:00\n",
      "#17 182.9 Using cached opentelemetry_api-1.28.2-py3-none-any.whl (64 kB)\n",
      "#17 182.9 Using cached opentelemetry_sdk-1.28.2-py3-none-any.whl (118 kB)\n",
      "#17 182.9 Using cached opentelemetry_semantic_conventions-0.49b2-py3-none-any.whl (159 kB)\n",
      "#17 182.9 Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
      "#17 182.9 Downloading pillow-11.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.4 MB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.4/4.4 MB 29.8 MB/s eta 0:00:00\n",
      "#17 182.9 Using cached protobuf-5.29.0-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "#17 182.9 Using cached pyparsing-3.2.0-py3-none-any.whl (106 kB)\n",
      "#17 182.9 Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "#17 182.9 Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "#17 182.9 Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "#17 182.9    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 763.0/763.0 kB 25.9 MB/s eta 0:00:00\n",
      "#17 182.9 Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "#17 182.9 Using cached sqlparse-0.5.2-py3-none-any.whl (44 kB)\n",
      "#17 182.9 Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "#17 182.9 Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "#17 182.9 Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "#17 182.9 Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "#17 182.9 Using cached Mako-1.3.6-py3-none-any.whl (78 kB)\n",
      "#17 182.9 Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "#17 182.9 Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "#17 182.9 Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "#17 182.9 Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "#17 182.9 Using cached google_auth-2.36.0-py2.py3-none-any.whl (209 kB)\n",
      "#17 182.9 Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "#17 182.9 Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "#17 182.9 Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "#17 182.9 Using cached pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "#17 182.9 Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "#17 182.9 Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "#17 182.9 Downloading wrapt-1.17.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
      "#17 182.9 Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "#17 182.9 Installing collected packages: pytz, mpmath, zipp, wrapt, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, sympy, sqlparse, smmap, six, pyyaml, pyparsing, pyasn1, pyarrow, protobuf, pillow, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, markdown, kiwisolver, joblib, itsdangerous, idna, greenlet, graphql-core, fsspec, fonttools, filelock, cycler, cloudpickle, click, charset-normalizer, certifi, cachetools, blinker, Werkzeug, triton, sqlalchemy, scipy, rsa, requests, python-dateutil, pyasn1-modules, nvidia-cusparse-cu12, nvidia-cudnn-cu12, Mako, Jinja2, importlib-metadata, gunicorn, graphql-relay, gitdb, deprecated, contourpy, scikit-learn, pandas, opentelemetry-api, nvidia-cusolver-cu12, matplotlib, graphene, google-auth, gitpython, Flask, docker, alembic, torch, opentelemetry-semantic-conventions, databricks-sdk, opentelemetry-sdk, mlflow-skinny, mlflow\n",
      "#17 182.9 Successfully installed Flask-3.1.0 Jinja2-3.1.4 Mako-1.3.6 MarkupSafe-3.0.2 Werkzeug-3.1.3 alembic-1.14.0 blinker-1.9.0 cachetools-5.5.0 certifi-2024.8.30 charset-normalizer-3.4.0 click-8.1.7 cloudpickle-3.1.0 contourpy-1.3.1 cycler-0.12.1 databricks-sdk-0.38.0 deprecated-1.2.15 docker-7.1.0 filelock-3.16.1 fonttools-4.55.0 fsspec-2024.10.0 gitdb-4.0.11 gitpython-3.1.43 google-auth-2.36.0 graphene-3.4.3 graphql-core-3.2.5 graphql-relay-3.2.0 greenlet-3.1.1 gunicorn-23.0.0 idna-3.10 importlib-metadata-8.5.0 itsdangerous-2.2.0 joblib-1.4.2 kiwisolver-1.4.7 markdown-3.7 matplotlib-3.9.3 mlflow-2.18.0 mlflow-skinny-2.18.0 mpmath-1.3.0 networkx-3.4.2 numpy-2.1.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opentelemetry-api-1.28.2 opentelemetry-sdk-1.28.2 opentelemetry-semantic-conventions-0.49b2 packaging-24.2 pandas-2.2.3 pillow-11.0.0 protobuf-5.29.0 pyarrow-18.1.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.2.0 python-dateutil-2.9.0.post0 pytz-2024.2 pyyaml-6.0.2 requests-2.32.3 rsa-4.9 scikit-learn-1.5.2 scipy-1.14.1 six-1.16.0 smmap-5.0.1 sqlalchemy-2.0.36 sqlparse-0.5.2 sympy-1.13.1 threadpoolctl-3.5.0 torch-2.5.1 tqdm-4.67.1 triton-3.1.0 typing-extensions-4.12.2 tzdata-2024.2 urllib3-2.2.3 wrapt-1.17.0 zipp-3.21.0\n",
      "#17 182.9 \n",
      "#17 182.9 done\n",
      "#17 182.9 #\n",
      "#17 182.9 # To activate this environment, use\n",
      "#17 182.9 #\n",
      "#17 182.9 #     $ conda activate custom_env\n",
      "#17 182.9 #\n",
      "#17 182.9 # To deactivate an active environment, use\n",
      "#17 182.9 #\n",
      "#17 182.9 #     $ conda deactivate\n",
      "#17 182.9 \n",
      "#17 184.2 Requirement already satisfied: gunicorn[gevent] in /miniconda/envs/custom_env/lib/python3.11/site-packages (23.0.0)\n",
      "#17 184.2 Requirement already satisfied: packaging in /miniconda/envs/custom_env/lib/python3.11/site-packages (from gunicorn[gevent]) (24.2)\n",
      "#17 184.6 Collecting gevent>=1.4.0 (from gunicorn[gevent])\n",
      "#17 184.8   Downloading gevent-24.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "#17 184.8 Collecting zope.event (from gevent>=1.4.0->gunicorn[gevent])\n",
      "#17 184.8   Downloading zope.event-5.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "#17 185.0 Collecting zope.interface (from gevent>=1.4.0->gunicorn[gevent])\n",
      "#17 185.0   Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (44 kB)\n",
      "#17 185.0 Requirement already satisfied: greenlet>=3.1.1 in /miniconda/envs/custom_env/lib/python3.11/site-packages (from gevent>=1.4.0->gunicorn[gevent]) (3.1.1)\n",
      "#17 185.0 Requirement already satisfied: setuptools in /miniconda/envs/custom_env/lib/python3.11/site-packages (from zope.event->gevent>=1.4.0->gunicorn[gevent]) (75.1.0)\n",
      "#17 185.0 Downloading gevent-24.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "#17 185.3    ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 6.8/6.8 MB 29.2 MB/s eta 0:00:00\n",
      "#17 185.3 Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
      "#17 185.3 Downloading zope.interface-7.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (259 kB)\n",
      "#17 185.5 Installing collected packages: zope.interface, zope.event, gevent\n",
      "#17 186.0 Successfully installed gevent-24.11.1 zope.event-5.0 zope.interface-7.2\n",
      "#17 186.0 WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\n",
      "#17 DONE 186.7s\n",
      "\n",
      "#18 [14/15] RUN chmod o+rwX /opt/mlflow/\n",
      "#18 DONE 0.1s\n",
      "\n",
      "#19 [15/15] RUN rm -rf /var/lib/apt/lists/*\n",
      "#19 DONE 0.2s\n",
      "\n",
      "#20 exporting to image\n",
      "#20 exporting layers\n",
      "#20 exporting layers 33.8s done\n",
      "#20 writing image sha256:8d4b669bc657f1076fe972266cba5231aaa0d23075c0fd70724360f965eb711c done\n",
      "#20 naming to docker.io/library/mlflow-model:pyfunc done\n",
      "#20 DONE 33.8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='mlflow models build-docker -m model -n mlflow-model:pyfunc --env-manager conda', returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = f\"mlflow models build-docker -m {MODEL_PATH} -n {IMAGE_NAME} --env-manager conda\"\n",
    "subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Inference\n",
    "In order to get predictions we need to run the docker image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTAINER_NAME = \"mlflow_server\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-12-01 23:43:30 +0000] [41] [INFO] Starting gunicorn 23.0.0\n",
      "[2024-12-01 23:43:30 +0000] [41] [INFO] Listening at: http://127.0.0.1:8000 (41)\n",
      "[2024-12-01 23:43:30 +0000] [41] [INFO] Using worker: sync\n",
      "[2024-12-01 23:43:30 +0000] [47] [INFO] Booting worker with pid: 47\n",
      "/tmp/ipykernel_429266/23546871.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "/tmp/ipykernel_429266/23546871.py:16: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n"
     ]
    }
   ],
   "source": [
    "cmd  = f'docker run -e GUNICORN_CMD_ARGS=\"--workers=1\"  -p {SERVE_PORT}:8080 --name {CONTAINER_NAME} {IMAGE_NAME}'\n",
    "\n",
    "process = subprocess.Popen(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172.17.0.1 - - [01/Dec/2024:23:43:40 +0000] \"POST /invocations HTTP/1.1\" 200 61 \"-\" \"python-requests/2.32.3\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'predictions': [[0.18097594380378723, 0.11162048578262329]]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = requests.post(f\"http://0.0.0.0:{SERVE_PORT}/invocations\", json={\"inputs\": input_example.numpy().tolist(), \"params\": {\"model\": \"model2\"}})\n",
    "result.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/01 23:43:49 INFO mlflow.models.container: Got sigterm signal, exiting.\n",
      "[2024-12-01 23:43:49 +0000] [41] [INFO] Handling signal: term\n",
      "[2024-12-01 23:43:49 +0000] [47] [INFO] Worker exiting (pid: 47)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a random function made for demo purposes\n",
      "mlflow_server\n",
      "mlflow_server\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='docker rm mlflow_server', returncode=0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd_stop = f\"docker stop {CONTAINER_NAME}\"\n",
    "subprocess.run(cmd_stop, shell=True)\n",
    "\n",
    "cmd_rm = f\"docker rm {CONTAINER_NAME}\"\n",
    "subprocess.run(cmd_rm, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Exporting the docker image\n",
    "In order to use the docker image in other machines, we need to compress it and upload it to the machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='docker save -o model.tar mlflow-model:pytorch', returncode=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = f\"docker save -o model.tar {IMAGE_NAME}\"\n",
    "subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Importing the docker image\n",
    "The target machine needs to have docker installed. Then we can load the image and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image: mlflow-model:pytorch\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='docker load -i model.tar', returncode=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = \"docker load -i model.tar\"\n",
    "subprocess.run(cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do inference in the target machine executing the same command as in step 6.2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
